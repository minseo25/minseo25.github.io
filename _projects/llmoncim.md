---
layout: page
title: LLM on Compute-In-Memory (CIM)
description: Making LLM Inference Feasible on Compute-in-Memory (CIM) hardware
img: assets/img/llmoncim.png
importance: 1
category: work
related_publications: false
---

<ul class="list-unstyled">
  <li>
    <i class="fa-solid fa-calendar mr-1"></i>
    <b>Period</b>: Jul 2025 - Present
  </li>
  <li>
    <i class="fa-solid fa-screwdriver-wrench mr-1"></i>
    <b>Tools</b>: PyTorch, Triton
  </li>
  <li class="mt-1">
    <i class="fa-brands fa-github mr-1"></i>
    <b>GitHub</b>:
    N/A
  </li>
  <li class="mt-1">
    <i class="fa-solid fa-link mr-1"></i>
    <b>URL</b>:
    N/A
  </li>
</ul>

---

<ul>
  <li>Make LLM inference feasible on compute-in-memory (CIM) hardware, which suffer from limited write endurance and only support integer.</li>
  <li>Ongoing project in Pallas Lab, UC Berkeley.</li>
</ul>
